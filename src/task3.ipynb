{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "def load_conll_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            token, label = line.split('\\t')\n",
    "            current_sentence.append(token)\n",
    "            current_labels.append(label)\n",
    "        else:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Load your labeled data\n",
    "file_path = 'labeled_data.conll'  \n",
    "sentences, labels = load_conll_dataset(file_path)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame({'tokens': sentences, 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Map labels to integers\n",
    "unique_labels = set(label for label_list in labels for label in label_list)\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "# Encode the labels\n",
    "encoded_labels = [[label_to_id[label] for label in label_list] for label_list in labels]\n",
    "\n",
    "# Train-test split\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({'tokens': train_sentences, 'labels': train_labels})\n",
    "test_dataset = Dataset.from_dict({'tokens': test_sentences, 'labels': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, padding=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word ids\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'])  # Initialize label ids\n",
    "        for j in range(len(label)):\n",
    "            if word_ids[j] is not None:  # Get the index of the token\n",
    "                label_ids[word_ids[j]] = label[j]  # Assign the label\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Set format for Trainer\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
